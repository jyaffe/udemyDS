{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Case Example\n",
    "\n",
    "You are given data from an audiobook app. Logically it relates to the audio versions of books only. Each customer in the database has made a purchase at least once. That's the condition to be included.\n",
    "\n",
    "We want to create a ML algorithm based on our data that predicts whether a customer will buy again from the audiobook company.\n",
    "\n",
    "The idea is that the audiobook company shouldn't spend its advertising budget targeting individuals who are unlikely to come back.\n",
    "\n",
    "If you focus marketing spend on customers who are likely to return and continue spending, you can improve sales and revenue figures.\n",
    "\n",
    "Our model will take several metrics and try to predict human behaviour. A side-benefit of this model is that it will show us which are the most important metrics that cause a customer to come back.\n",
    "\n",
    "Having the data and the technology to identify prospective customers creates a lot of value and growth opportunities. It is one of the better applications of data science.\n",
    "\n",
    "The data is a csv file, with each row representing a person. \n",
    "\n",
    "- Customer ID - ID is like a name.\n",
    "- Book length (overall) is the sum of lengths of all purchases.\n",
    "- Book length (average) is the average length per book.\n",
    "- Number of purchases is not explicit, but is calculated from the book length variables if required.\n",
    "- Price (overall and average) act in the same way as book length. NB> price variable is almost always a good predictor.\n",
    "- Review is a boolean that shows if the customer left a review. It shows engagement with the platform. \n",
    "- Review 10/10 meeasures the review of the customer on a scale of 1-10. \n",
    "- The two review columns show an early step for preprocessing. If a customer has not left a review, they have a 0 bool value and no entry for 10/10 rating. It is possible to combine the two fields. One way would be to fill in missing values with the average review score for those that were rated. That average would be the status quo of book rating. A review of above this average indicates above average feelings towards content on the platform.\n",
    "- Minutes listened is measure of engagement\n",
    "- Completion is total minutes listened / book length (overall)\n",
    "- Support requests show the total number of support requests a customer has made. (A measure of engagement)\n",
    "- Last interaction vs first purchase date. the bigger the difference, the bigger the engagement. If the value is 0, the customer has never accessed what they bought, or just used it on the first day only.\n",
    "\n",
    "It is necessary to ask how the data was gathered. This data was gathered from the audiobook app. It represents 2 years worth of engagement.\n",
    "\n",
    "We are doing supervised learning, so we need to set targets. 1 if the customer converted, 0 if they didnt. \n",
    "\n",
    "What does it mean to convert??\n",
    "\n",
    "We have taken an extra 6 months of data after that 2 year period to check if a customer has converted. So overall we have 2 years and 6 months of data. \n",
    "\n",
    "The inputs are the 2 years of data. The targets are whether they bought a book in the 6 months following. If they did, it would be bool value of 1, if they didnt, it would be a bool value of 0.\n",
    "\n",
    "That is how the targets column is created. \n",
    "\n",
    "We need to create a ML algorithm that can predict if a customer will buy again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Business Case Action Plan\n",
    "\n",
    "- preprocess the data\n",
    "    - balance the dataset\n",
    "    - divide the dataset in training, validation, test\n",
    "    - save the data in a tensor friendly format\n",
    "- create the machine learning algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the Dataset\n",
    "\n",
    "What accuracy do you expect from a model that is meant to classify photos of cats and dogs. \n",
    "\n",
    "- 70% is not too bad\n",
    "- 80% is good\n",
    "- 90% is very good for beginners and useful for most problems\n",
    "\n",
    "Imagine a model that takes animal photos and outputs only cats. No matter what you feed to the algorithm, it will always output cat as the answer. This is a bad model.\n",
    "\n",
    "Imagine that 90% of the photos in a dataset was cats and 10% of them are dogs. The model would always output cats. But 90% of them are cats, so what is the accuracy? It is 90%.\n",
    "\n",
    "The ML model is trying to reduce the losses, and if most outputs are cats, it is safe to predict that everything is a cat.\n",
    "\n",
    "If this same dataset had a model that had 80% accuracy, this would be an awful model, as one that said just cats would be right 90% of the time and so much better.\n",
    "\n",
    "When you talk about the balance of cats and dogs in the original dataset, you are talking about PRIORS being unbalanced. The Priors are balanced when 50% are cats and 50% are dogs.\n",
    "\n",
    "When you have unbalanced priors, a ML algorithm might quickly learn that one outcome is more likely than the other and that will skew the results. \n",
    "\n",
    "If we have three classes, cats dogs and horses, balancing would mean each would be 1/3. and so on...\n",
    "\n",
    "By exploring the dataset in this case study, you can see that most customers did not convert in the following 6 months, which means it is an unbalanced dataset. \n",
    "\n",
    "The dataset needs to be balanced by counting the total number of target 1s and matching the same number of target 0s to them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example. Audiobooks\n",
    "\n",
    "#### Preprocess the data. Balance the dataset. Create 3 datasets: training, validation, and test. Save the newly created sets in a tensor friendly format (e.g. *.npz)\n",
    "\n",
    "### Extract the data from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# sklearn's preprocessing capabilities means one line of code can drastically help\n",
    "\n",
    "raw_csv_data = np.loadtxt('/Users/yafja/Google Drive/Jack/Udemy/DataSci/The Data Science Course 2018 - All Resources/Part_7_Deep_Learning/S51_L356/Audiobooks_data.csv',delimiter=',')\n",
    "\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(targets_all))\n",
    "zero_targets_counter = 0\n",
    "indices_to_remove = []\n",
    "\n",
    "# the shape of targets_all on axis=0, is basically the length of the vector.\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "            \n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all,indices_to_remove,axis = 0)\n",
    "targets_equal_priors = np.delete(targets_all,indices_to_remove,axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardise the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1788.0 3579 0.49958088851634536\n",
      "218.0 447 0.48769574944071586\n",
      "231.0 448 0.515625\n"
     ]
    }
   ],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "print(np.sum(train_targets),train_samples_count,np.sum(train_targets)/train_samples_count)\n",
    "print(np.sum(validation_targets),validation_samples_count,np.sum(validation_targets)/validation_samples_count)\n",
    "print(np.sum(test_targets),test_samples_count,np.sum(test_targets)/test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three sets are balanced now.\n",
    "\n",
    "### Save the three datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train',inputs=train_inputs,targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation',inputs=validation_inputs,targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test',inputs=test_inputs,targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is preprocessed now. Each time we run the code we will get different data as the sampling and shuffling will happen differently. \n",
    "\n",
    "This code is reusable for any dataset that has two classes.\n",
    "\n",
    "If there is a dataset that has more than two classes, the only change needs to be in the balancing part, where you need to balance it out for x classes rather than for 2.\n",
    "\n",
    "It does make more sense to shuffle the indices prior to balancing the dataset, so that could be changed in this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemyDS",
   "language": "python",
   "name": "udemyds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
