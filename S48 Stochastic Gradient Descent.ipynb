{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation\n",
    "\n",
    "The algorithms we will use to vary our model's parameters. \n",
    "\n",
    "\n",
    "## Gradient desecent\n",
    "\n",
    "The GD iterates over the whole training set before updating the weights. Each update is small, which is the whole concept of this model, driven by the value of the learning rate. Choosing a high learning rate, jeopardises the accuracy of the algorithm.\n",
    "\n",
    "This is a slow learning algoritm.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "This is similar to gradient descent, but it updates the weights multiple times during a single epoch.\n",
    "\n",
    "It uses the concept of batching, splitting the dataset into n batches. The weights are updated with every batch, rather than every epoch.\n",
    "\n",
    "It is the same method generally as the gradient descent, but much faster.\n",
    "\n",
    "We do loose a bit of accuracy compared to GD, but it is good enough to make it the industry standard.\n",
    "\n",
    "One of the best reasons for this, is that the training can be parrelised by moving batches onto differnet CPU cores.\n",
    "\n",
    "This might actually be called mini-batch GD, but practitioners often refer to it as SGD.\n",
    "\n",
    "\n",
    "## Problems with Gradient Descent\n",
    "\n",
    "The GD and the SGD are the logical ways to train our models. \n",
    "\n",
    "A single batch GD is slow, but eventually reaches the minimum value in a consistent manner.\n",
    "\n",
    "An SGD would be much faster, but give us an approximate answer rather than the exact one. The savings in computation speed make it well-worth the tradeoff.\n",
    "\n",
    "In real life, loss functions are not regular shaped graphs. They can have multiple minima (lows of curves). There can be multiple local minimum points which are not close to the global minimum. Each local minimum is a suboptimal solution to the optimisation problem.\n",
    "\n",
    "Gradient Descent is prone to this issue. Often it falls into the local minimum closest to its starting point, rather than finding the global minimum.\n",
    "\n",
    "A higher learning rate may miss the first local minimum and find itself in the global one, whereas a lower learning rate is unlikely to make it out of the local one. The large learning rate would still likely oscillate and never find the bottom.\n",
    "\n",
    "Remedies can be applied to reach the desired results.\n",
    "\n",
    "## Momentum\n",
    "\n",
    "GD and SGD are good ways to train the models. We should just extend them.\n",
    "\n",
    "Momentum is a great extension to compensate. Imagine a ball rolling down a hill. If it finds a local minimum, the momentum might make it continue and exit the local dip before settling right at the bottom.\n",
    "\n",
    "This is done by measuring the speed of descent so far. The method is to check the speed of the descent a moment before the current one.\n",
    "\n",
    "Momentum takes the current update and subtracts the update from a moment ago.\n",
    "\n",
    "To make sure the update from a moment ago is appropriately adjusted for the fact that it is less important than the update that just happened, a hyperparameter is added which acts like a weighting. \n",
    "\n",
    "Usually this weighting is $\\alpha = 0.9$\n",
    "\n",
    "$ w = \\underset{Current\\;Update}{\\underbrace{w(t) - \\eta\\frac{\\partial L}{\\partial w}(t)}} - \\underset{Last\\;Update}{\\underbrace{\\alpha\\eta\\frac{\\partial L}{\\partial w}(t-1)}}$\n",
    "\n",
    "## Learning Rate Schedules\n",
    "\n",
    "Hyperparameters (pre-set by us):\n",
    "- width\n",
    "- depth\n",
    "- learning rate ($\\eta$)\n",
    "- batch size\n",
    "- momentum coefficient ($\\alpha$)\n",
    "- decay coefficient (c)\n",
    "\n",
    "Parameters (found by optimising):\n",
    "- weights (w)\n",
    "- biases (b)\n",
    "\n",
    "The learning rate must be small enough so we gently descent, instead of oscillating or diverging. It must be big enough so the optimisation occurs in a reasonable amount of time.\n",
    "\n",
    "Small enough and big enough are too vague.\n",
    "\n",
    "A learning rate schedule can handle both issues:\n",
    "- We start from a high initial learning rate\n",
    "- At some point we lower the rate to avoid oscillation\n",
    "- Around the end we pick a very small rate to get a precise answer\n",
    "\n",
    "In practise, the way to implement a learning rate schedule is to set a predetermined piecewise learning rate:\n",
    "- Frist 5 epochs $\\eta = 0.1$\n",
    "- Next 5 epochs $\\eta = 0.01$\n",
    "- Until the end $\\eta = 0.001$\n",
    "\n",
    "This causes the loss function to converge much faster.\n",
    "\n",
    "However, it is too simple still. It is very crude and it requires us to know roughly how many epochs it requires to converge on the answer. \n",
    "\n",
    "A second approach is the exponential schedule. It smoothly decays the learning rate:\n",
    "- We start from a high value $\\eta_0 = 0.1$\n",
    "- Then we update the learning rate at each epoch using the rule $\\eta = \\eta_0e^{-n/c}$, where n is the current epoch and c is a constant.\n",
    "- C is a hyperparameter, usualy the same order of magnitude as the number of epochs needed to minimise the loss. Usually a C of around 20 or 30 does. \n",
    "- The value of C makes less impact than the presence of the learning rate schedule itself.\n",
    "\n",
    "## Advanced Learning Rate Schedules\n",
    "\n",
    "AdaGrad\n",
    "\n",
    "RMSProp\n",
    "\n",
    "In tensorflow you can choose either of these. When doing ML, you must be able to choose the best methods to train your model.\n",
    "\n",
    "AdaGrad stands for adaptive gradient algorithm. It dynamically varies the learning rate at each update and for each weight individually in a monotonous way, i.e. in one direction. The learning rate is based on the training itself, rather than being set to a schedule beforehand. Each individual weight keeps track of its own step change functions. Different weights do not reach their optimal weight simultaneously.\n",
    "\n",
    "RMSProp stands for root mean square propogation. It is similar to AdaGrad but requires another hyperparameter, $\\beta$ which is usually around 0.9. This new hyperparameter and some other algorithmic changes means that it is not a monotonous function, it can adapt upwards and downwards. \n",
    "\n",
    "Both methods are logical and smart, but there is a third one which combines the two and is even better.\n",
    "\n",
    "\n",
    "## ADAM (Adaptive Moment Estimation)\n",
    "\n",
    "Combines momentum and learning rate schedules.\n",
    "\n",
    "This is the most advanced optimiser applied in practice, came out in 2015.\n",
    "\n",
    "Adam introduces Momentum into RMSProp.\n",
    "\n",
    "$\\Delta w_i(t)\\; =\\; -\\frac{\\eta}{\\sqrt{G_i(t)}\\:+\\:\\epsilon}M_i(t)$ \n",
    "\n",
    "Adam is a great method to work with due to its advanced capabilities.\n",
    "\n",
    "As with all science, data science is a long chain of academic research built on top of each other.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemyDS",
   "language": "python",
   "name": "udemyds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
