{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset\n",
    "\n",
    "This is the 'Hello World' of ML training.\n",
    "\n",
    "The MNIST dataset consists of 70,000 handwritten digits. Since we have 10 digits, we have 10 classes from 0 to 9.\n",
    "\n",
    "Our challenge is to create an algorithm that takes an image and correctly determines what digit is shown in that image. \n",
    "\n",
    "## Solution Outline\n",
    "\n",
    "Each image in the MNIST dataset is 28 pixels by 28 pixels. \n",
    "\n",
    "It is on a greyscale so the pixel values are between 0 and 255. \n",
    "\n",
    "We can think about the problem as a 28x28 matrix with input values between 0 and 255. \n",
    "\n",
    "0 corresponds to purely black and a 255 to purely white.\n",
    "\n",
    "As each photo is 28x28 pixels, the total pixel number is 784.\n",
    "\n",
    "The approach for deep neural networks is to 'flatten' each image into a vector 784 x 1.\n",
    "\n",
    "So for each image you would have 784 inputs to the neural network.\n",
    "\n",
    "Then we will linearly combine the input layer and add a non-linearity to create the first hidden layer. This model will have two hidden layers. Two are enough to produce a model for this with very good accuracy.\n",
    "\n",
    "We then need to produce the output layer. There are 10 digits, therefore 10 classes and so there will be 10 output units after the second hidden layer. \n",
    "\n",
    "The output will then be compared to the targets. We will use one-hot encoding for both the outputs and the targets.\n",
    "\n",
    "I.e. the digit 0 will be represented by [1,0,0,0,0,0,0,0,0,0] and 5 will be represented by [0,0,0,0,0,1,0,0,0,0].\n",
    "\n",
    "Since we want to see the probability of a digit being rightfull labelle we will use a softmax activation function on the output layer.\n",
    "\n",
    "### Action plan:\n",
    "- prepare our data and preprocess it. create training, validation and test datasets\n",
    "- outline the model and choose the activation functions\n",
    "- set the appropriate advanced optimisers and the loss function\n",
    "- make it learn\n",
    "- test the accuracy of the model\n",
    "\n",
    "## Import the relevant packages\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds # Tensorflow datasets has a lot of data ready for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Loading the dataset with flag `as_supervised = True` loads the data in a 2-tuple structure.\n",
    "\n",
    "Loading the dataset with flag `with_info = True` provides a tuple containing info about version, features, # samples of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset,mnist_info = tfds.load(name='mnist',with_info=True,as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'],mnist_dataset['test']\n",
    "\n",
    "# this dataset does not already have a validation dataset so we will take 10% of the training set\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples,tf.int64)\n",
    "# cast converts a variable into a given data type\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples,tf.int64)\n",
    "\n",
    "def scale(image,label):\n",
    "    image = tf.cast(image,tf.float32)\n",
    "    image /= 255.\n",
    "    return image,label\n",
    "\n",
    "# there is a tensor flow function called dataset.map that applies a custom\n",
    "# transformation to a given dataset. it takes as input a function which\n",
    "# determines the transformation. \n",
    "# This map function only takes functions that work with a variable and a label.\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling is something often done at the preprocessing stage. It keeps the same information, but puts it in a different order. Like shuffling cards. \n",
    "\n",
    "It's possible that the targets are stored in a descending order, resulting in the first x batches having only one value for target and other batches having only other values. Shuffling protects against this. Otherwise the stochastic gradient descent algorithm would not work properly. \n",
    "\n",
    "You need to set a buffer size which limits the amount of data being shuffled in one go. Otherwise the data might be too large and take up the entire memory of the machine and cause issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples) # model expects this in batch format so use the samples num\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "# Validation data must have same shape and format as the train and test data\n",
    "# The MNIST data is iterable and in 2-tuple format (as_supervised=True)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Outline the model\n",
    "\n",
    "784 inputs\n",
    "10 outputs, one for each digit\n",
    "we will work with 2 hidden layers of 50 elements each\n",
    "\n",
    "The optimal width and depth for this problem is unknown, but it is safe to assume the initial values of 4 layers for depth and 50 for width are suboptimal. NB: I don't actually know what the width and depth values are, just assumed those two values for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 100\n",
    "# underlying assumption is that all hidden layers are same size\n",
    "# it is possible to vary hidden layer size\n",
    "\n",
    "# the model below shows how each layer is built. I imagine you could be clever about adding multiple hidden layers.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size,activation='relu'), # activation function chosen because it is known to be good at this problem\n",
    "    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size,activation='softmax') # output layer for this problem must turn outputs into probabilities, which softmax does.    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the optimiser and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "What happens inside an epoch?\n",
    "- At the beginning of each epoch, the training loss will be set to 0\n",
    "- The algorithm will iterate over a preset number of batches, all from train_data\n",
    "- The weights and biases will be updated as many times as there are batches\n",
    "- We will get a value for the loss function, indicating how the training is going\n",
    "- We will also see a training accuracy\n",
    "- At the end of the epoch, the algorithm will forward propogate the whole validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 3s - loss: 0.3301 - accuracy: 0.9048 - val_loss: 0.1548 - val_accuracy: 0.9553\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.1399 - accuracy: 0.9584 - val_loss: 0.1106 - val_accuracy: 0.9665\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.0987 - accuracy: 0.9702 - val_loss: 0.0879 - val_accuracy: 0.9720\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.0763 - accuracy: 0.9766 - val_loss: 0.0838 - val_accuracy: 0.9738\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.0614 - accuracy: 0.9807 - val_loss: 0.0585 - val_accuracy: 0.9825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14942a400>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "model.fit(train_data,epochs=NUM_EPOCHS,validation_data=(validation_inputs,validation_targets),verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreases with every epoch which is great. They dont change enormously between epochs as within each epoch there are 540 different weights and biases in the hidden layers. \n",
    "\n",
    "The accuracy shows in what % of the cases our outputs were equal to the targets. Logically they increase over time as the model gets better.\n",
    "\n",
    "We usually keep an eye on the validation loss (or set early stopping mechanisms) to determine whether the model is overfitting.\n",
    "\n",
    "The validation_accuracy is the TRUE ACCURACY OF THE MODEL.\n",
    "\n",
    "97% is already a great result, but if the hyperparameters are changed can it get better?\n",
    "\n",
    "Hidden layer size can be changed from 50 to 100 and the model re-run...\n",
    "\n",
    "With 100, the validation accuracy is now at 98.25% which is much better.\n",
    "\n",
    "TensorFlow think that models should be able to get to 99% and above in many circumstances.\n",
    "\n",
    "## Test the model\n",
    "\n",
    "The validation accuracy is still only on validation data. We might have overfit still. \n",
    "\n",
    "We need to test it on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9735\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08. Test accuracy: 97.35%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss,test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has a 97.35% accuracy.\n",
    "\n",
    "After you test the model, you are no longer allowed to change it. The test data will no longer be a dataset that the model has never seen. \n",
    "\n",
    "The main point of the test dataset is to simulate model deployment. \n",
    "\n",
    "Getting a test accuracy very close to the validation accuracy is a sign that we have not got overfitting.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemyDS",
   "language": "python",
   "name": "udemyds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
