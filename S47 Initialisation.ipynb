{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation\n",
    "\n",
    "It is the process in which we set the initial values of weights. An inappropriate initialisation cause cause a model to work incorrectly. \n",
    "\n",
    "If you were to initialise the weights and biases in such a way that tey are all equal to a constant, e.g. 5, the hidden units would be completely symmetrical with respect to the inputs.\n",
    "\n",
    "If all the weights are equal, there is no reason for the algorithm to learn that there is differentiation between anything. \n",
    "\n",
    "When backpropogating, the algorithm wont be able to distinguish between the nodes in the net. Some optimisation would still take place, but the weights would generally remain useless. \n",
    "\n",
    "## Types of simple initialisations\n",
    "\n",
    "A simple approach would be to initialise weights randomly within a small range. \n",
    "    - this chooses the values randomly, but in a uniform way\n",
    "    - each value is as likely as any other to be selected\n",
    "    \n",
    "Equal probability is important.\n",
    "\n",
    "A second approach is a normal initialiser. Picking the numbers from a small normal distribution.\n",
    "    - as it follows the normal distribution, values around 0 are more common\n",
    "    - it is common when using a normal-distribution to only select values within the standard deviation of 0.1\n",
    "    \n",
    "    \n",
    "Both methods were used a lot until recently, but always been sub-optimal. This is something to do with creating linear relationships in the sigmoid activation function, by having the values close to the value of 0, which is where the sigmoid function behaves more like a straight line than elsewhere. Non-linearity is important in neural nets. The algorithm may not learn properly. \n",
    "\n",
    "Recently, a better method has arisen...\n",
    "\n",
    "## Xavier Initialisation\n",
    "\n",
    "It is also known as the Glorot Initialisation. Xavier Glorot is the academic who proposed this method.\n",
    "\n",
    "There are both a uniform xavier initialisation and a normal xavier initialisaion.\n",
    "\n",
    "The main idea is that the method used for initialisation is not so important, the number of inputs and outputs are. \n",
    "\n",
    "Uniform Xavier initialisation:\n",
    "- draw each weight, w, from a random uniform distribution in [-x,x] for \n",
    "- $x\\;=\\;\\sqrt{\\frac{6}{inputs\\:+\\:outputs}}$\n",
    "\n",
    "Normal Xavier initialisation:\n",
    "- draw each weight, w, from a normal distribution with a mean of 0, and a standard deviation of\n",
    "- $ \\sigma = \\sqrt{\\frac{2}{inputs\\:+\\:outputs}}$\n",
    "\n",
    "The numerators 2 and 6 vary across sources but the theory is the same.\n",
    "\n",
    "In tensorflow, this is the default initialiser. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemyDS",
   "language": "python",
   "name": "udemyds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
