{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Preprocessing refers to any manipluation of the dataset before running it through the model.\n",
    "\n",
    "We have already seen some preprocessing. In the tensorflow intro we created a np.savez file and all the training came from there.\n",
    "\n",
    "In this section we will focus on data transformation rather than reordering.\n",
    "\n",
    "We preprocess for a number of reasons.\n",
    "- compatability is one, ie tensor flow takes tensors rather than excel or csv files\n",
    "- orders of magnitude, if one input is a lot larger than another, they have to be adjusted to be relevant relative to each other, especially when combining with matrix mathematics\n",
    "- generalisation, problems that seem different can be solved by the same models. if you can generalise data you can possibly use models built before\n",
    "\n",
    "\n",
    "## Types of Basic Preprocessing\n",
    "\n",
    "Often we are interested in a relative value, often when talking about stock prices. I.e. % change in stock prices is often important.\n",
    "\n",
    "Relative metrics are especially useful when we have time-series data.\n",
    "\n",
    "We can transform these relative changes into logarithms, as they have clearer relationships and homogenous variance. The computation is often faster and have lower order of magnitudes. \n",
    "\n",
    "## Standardisation\n",
    "\n",
    "The most challenging problem when working with numerical data is in working with different magnitutes.\n",
    "\n",
    "The fix for magnitude issues is standardisation, often called feature scaling and normalisation. \n",
    "\n",
    "This is the process of transforming data into a standard scale.\n",
    "\n",
    "A common way of doing this is by subtracting the mean and dividing by the standard deviation:\n",
    "\n",
    "$ standardised\\: variable\\: =\\: \\frac{x - \\mu}{\\sigma}$\n",
    "\n",
    "Regardless of the dataset, this will always result in a dataset with a mean of zero and a standard deviation of one. \n",
    "\n",
    "Normalisation refers to a few methods. One such method is by taking a matrix and converting it into a unit length vector using the L1- or L2-norm. \n",
    "\n",
    "Another method is PCA (principal components analysis) which is a dimension reduction technique used to combine several variables into a bigger (latent) variable. For example if you have data on religion, voting histroy and upbringing you can associate them all in a single value which might refer to attitude towards immigration, with a mean of zero and a standard deviation of one.\n",
    "\n",
    "Whitening is another technique sometimes used for preprocessing. It is often used after PCA and it removes most of the underlying correlations between data points. Whitening can be useful when the underlying data should be uncorrelated but that is not reflected in the observations. \n",
    "\n",
    "There are a lot more and each strategy is problem specific. \n",
    "\n",
    "## Preprocessing Categorical Data\n",
    "\n",
    "Most of what we have seen are examples of numerical data. Often we must deal with categorical data, i.e. groups or categories, such as cats or dog values. The ML algorithm only takes numbers, so we need to be able to convert cat or dog to a number, or a tensor.\n",
    "\n",
    "How do you convert categories to numbers? One solution is to say, cat = 1, dog =2. Unfortunately this implies that there is some order, which is not true. 2 x cat is not a dog. \n",
    "\n",
    "How to encode categories in a way that is useful for an ML algo.\n",
    "\n",
    "There are two main methods here:\n",
    "- one-hot encoding\n",
    "- binary encoding\n",
    "\n",
    "## Binary and One-Hot Encoding\n",
    "\n",
    "Binary encoding starts by issuing random numbers to each category. You then convert those numbers into binary numbers. 1 = 01, 2=10, 3 = 11. You then take each binary digit as a variable so number 1 has a 0 for var1 and a 1 for var2. Number 2 has a 1 for var1 and a 0 for var2. 3 has a 1 for var1 and a 1 for var2.\n",
    "\n",
    "There are still some implied correlations between them however, for insance 1 and 3 seem correlated on var1 and 2 and 3 seem correlated on var2. 1 and 2 seem the opposite of each other. \n",
    "\n",
    "Therefore binary encoding is an improvement of normal, but not great.\n",
    "\n",
    "One-Hot is simple and widely adopted. It starts by creating as many columns as there are possible values. I.e. for 3 products there must be 3 variables or columns. Then a product gets a 1 in the column that represents itself and a zero in all others. Each product has 1 variable set to a 1, all others at 0. This means the products are uncorrelated and unequivocal.\n",
    "\n",
    "The problem with one-hot encoding is that it requires a lot of new variables. Eg. IKEA offers 12,000 products, and we wont want to include 12,000 extra variables in our models. If we used binary encoding for this problem there would only be 16 variables, due to the scalability of binary numbers vs the other method. This is exponentially lower than requirements for one-hot. In these circumstances you must use binary even though it would include unnecessary correlations between some products.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemyDS",
   "language": "python",
   "name": "udemyds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
